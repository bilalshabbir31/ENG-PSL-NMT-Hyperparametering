{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PSL Hyperparameter Tuning [Eng-PSL].ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPsjQOyA7NOS",
        "outputId": "aea4971d-7d31-4eb1-da36-536a188229fb"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRLg3sZ871zY",
        "outputId": "efcb8201-0ae7-4c6b-c96d-c8a868101396"
      },
      "source": [
        "pip install keras-tuner --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.1.0-py3-none-any.whl (98 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 26.5 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 98 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.7.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (3.0.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.42.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.1.0 kt-legacy-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLcaeEdF74YO"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import string\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dense,Embedding,RepeatVector,TimeDistributed,GRU\n",
        "from keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Markdown, display\n",
        "from keras.layers import Dropout\n",
        "from tensorflow import keras\n",
        "import keras_tuner as kt\n",
        " \n",
        "def printmd(string):\n",
        "    # Print with Markdowns    \n",
        "    display(Markdown(string))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "DGpqEmGB779d",
        "outputId": "38c55223-2507-48b3-e92a-7c2c980130c2"
      },
      "source": [
        "# How many sentences will be used\n",
        "# Limit the sentences to 10.000 on Kaggle to avoid exceding the\n",
        "# available RAM space\n",
        "# Build a generator to avoid this issue\n",
        "\n",
        "total_sentences = 85221\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_excel(\"/content/drive/MyDrive/data.xlsx\", nrows = total_sentences)\n",
        "\n",
        "# What proportion of the sentences will be used for the test set\n",
        "test_proportion = 0.3\n",
        "train_test_threshold = int( (1-test_proportion) * total_sentences)\n",
        "\n",
        "printmd(f'## {total_sentences} \"parallel sentences\" will be loaded (original sentence + its translation)')\n",
        "printmd(f'## {train_test_threshold} \"parallel sentences\" will be used to train the model')\n",
        "printmd(f'## {total_sentences-train_test_threshold} \"parallel sentences\" will be used to test the model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## 85221 \"parallel sentences\" will be loaded (original sentence + its translation)",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## 59654 \"parallel sentences\" will be used to train the model",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## 25567 \"parallel sentences\" will be used to test the model",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "dzEZetfv7-o9",
        "outputId": "f9202c40-80f4-4201-a047-cfb8f4809462"
      },
      "source": [
        "# Shuffle the dataset\n",
        "dataset = dataset.sample(frac=1, random_state=0)\n",
        "dataset.iloc[1000:1010]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>PSL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42870</th>\n",
              "      <td>The chief was deciding to be a good man.</td>\n",
              "      <td>Was The chief be a good man decides to now.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3261</th>\n",
              "      <td>I have an orange and an apple.</td>\n",
              "      <td>I an orange an apple have.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73922</th>\n",
              "      <td>I had hoped to meet her there.</td>\n",
              "      <td>was I there her meet hope full.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84184</th>\n",
              "      <td>I know that Shazim is agnostic.</td>\n",
              "      <td>I know that Shazim agnostic.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79760</th>\n",
              "      <td>He is a director and should be treated as such.</td>\n",
              "      <td>He director such treat.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61886</th>\n",
              "      <td>I can not believe Anees said yes.</td>\n",
              "      <td>I believe not was Anees say.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65728</th>\n",
              "      <td>Waasif did not follow the rules.</td>\n",
              "      <td>was Waasif rules follow not.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36340</th>\n",
              "      <td>It sounds very strange to me.</td>\n",
              "      <td>It me very strange sounds.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4064</th>\n",
              "      <td>Balam took off his belt.</td>\n",
              "      <td>was Balam his belt take off.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24879</th>\n",
              "      <td>We have just got to keep going.</td>\n",
              "      <td>We just go keep get full.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               English                                          PSL\n",
              "42870         The chief was deciding to be a good man.  Was The chief be a good man decides to now.\n",
              "3261                    I have an orange and an apple.                   I an orange an apple have.\n",
              "73922                   I had hoped to meet her there.              was I there her meet hope full.\n",
              "84184                  I know that Shazim is agnostic.                 I know that Shazim agnostic.\n",
              "79760  He is a director and should be treated as such.                      He director such treat.\n",
              "61886                I can not believe Anees said yes.                 I believe not was Anees say.\n",
              "65728                 Waasif did not follow the rules.                 was Waasif rules follow not.\n",
              "36340                    It sounds very strange to me.                   It me very strange sounds.\n",
              "4064                          Balam took off his belt.                 was Balam his belt take off.\n",
              "24879                  We have just got to keep going.                    We just go keep get full."
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FnkUNmN8A6S"
      },
      "source": [
        "def clean(string):\n",
        "    # Clean the string\n",
        "    string = string.replace(\"\\u202f\",\" \") # Replace no-break space with space\n",
        "    string = string.lower()\n",
        "    \n",
        "    # Delete the punctuation and the numbers\n",
        "    for p in punctuation + \"«»\" + \"0123456789\":\n",
        "        string = string.replace(p,\" \")\n",
        "        \n",
        "    string = re.sub('\\s+',' ', string)\n",
        "    string = string.strip()\n",
        "           \n",
        "    return string\n",
        "\n",
        "# Clean the sentences\n",
        "dataset[\"English\"] = dataset[\"English\"].apply(lambda x: clean(x))\n",
        "dataset[\"PSL\"] = dataset[\"PSL\"].apply(lambda x: clean(x))\n",
        "\n",
        "# Select one part of the dataset\n",
        "dataset = dataset.values\n",
        "dataset = dataset[:total_sentences]\n",
        "\n",
        "# split into train/test\n",
        "train, test = dataset[:train_test_threshold], dataset[train_test_threshold:]\n",
        "\n",
        "# Define the name of the source and of the target\n",
        "# This will be used in the outputs of this notebook\n",
        "source_str, target_str = \"English\", \"PSL\"\n",
        "\n",
        "# The index in the numpy array of the source and of the target\n",
        "idx_src, idx_tar = 0,1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "id": "A1f0zwbB8D2b",
        "outputId": "0905dd36-a3af-4359-e1a0-8a9941cffdfa"
      },
      "source": [
        "def create_tokenizer(lines):\n",
        "    # fit a tokenizer\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        " \n",
        "def max_len(lines):\n",
        "    # max sentence length\n",
        "    return max(len(line.split()) for line in lines)\n",
        "\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    # encode and pad sequences\n",
        "    X = tokenizer.texts_to_sequences(lines) # integer encode sequences\n",
        "    X = pad_sequences(X, maxlen=length, padding='post') # pad sequences with 0 values\n",
        "    return X\n",
        " \n",
        "def encode_output(sequences, vocab_size):\n",
        "    # one hot encode target sequence\n",
        "    ylist = list()\n",
        "    for sequence in sequences:\n",
        "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "        ylist.append(encoded)\n",
        "    y = np.array(ylist)\n",
        "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "    return y\n",
        " \n",
        "# Prepare target tokenizer\n",
        "tar_tokenizer = create_tokenizer(dataset[:, idx_tar])\n",
        "tar_vocab_size = len(tar_tokenizer.word_index) + 1\n",
        "tar_length = max_len(dataset[:, idx_tar])\n",
        "printmd(f'\\nTarget ({target_str}) Vocabulary Size: {tar_vocab_size}')\n",
        "printmd(f'Target ({target_str}) Max Length: {tar_length}')\n",
        "\n",
        "# Prepare source tokenizer\n",
        "src_tokenizer = create_tokenizer(dataset[:, idx_src])\n",
        "src_vocab_size = len(src_tokenizer.word_index) + 1\n",
        "src_length = max_len(dataset[:, idx_src])\n",
        "printmd(f'\\nSource ({source_str}) Vocabulary Size: {src_vocab_size}')\n",
        "printmd(f'Source ({source_str}) Max Length: {src_length}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "\nTarget (PSL) Vocabulary Size: 8089",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Target (PSL) Max Length: 14",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "\nSource (English) Vocabulary Size: 9256",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "Source (English) Max Length: 12\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCWD-Z9M8D43"
      },
      "source": [
        "## Glove Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a22N9-gE8D7m",
        "outputId": "c52519a4-cd7e-4552-e548-734a973b46e8"
      },
      "source": [
        "# load embeding for text file\n",
        "embeddings_index = dict()\n",
        "f = open('/content/drive/MyDrive/Saved Models/glove.6B.300d.txt')\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnDxCBZt8D-_"
      },
      "source": [
        "dims=100\n",
        "flag=1\n",
        "word_index = {w: i for i, w in enumerate(embeddings_index, 1)}\n",
        "embedding_matrix = np.zeros((len(word_index)+1, dims))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector[:dims]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ7NyGkp8NjP"
      },
      "source": [
        "# Prepare training data\n",
        "trainX = encode_sequences(src_tokenizer, src_length, train[:8000, idx_src])\n",
        "trainY = encode_sequences(tar_tokenizer, tar_length, train[:8000, idx_tar])\n",
        "trainY = encode_output(trainY, tar_vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vG0ioC48Nnt"
      },
      "source": [
        "# Prepare test data\n",
        "testX = encode_sequences(src_tokenizer, src_length, test[:500, idx_src])\n",
        "testY = encode_sequences(tar_tokenizer, tar_length, test[:500, idx_tar])\n",
        "testY = encode_output(testY, tar_vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu1qJI9XJoh4"
      },
      "source": [
        "# #providing the range for hidden layers  \n",
        "#     for i in range(hp.Int('num_of_layers',1,3)):         \n",
        "#         #providing range for number of neurons in hidden layers\n",
        "#         model.add(Dense(units=hp.Int('num_of_neurons'+ str(i),min_value=64,max_value=312,step=64),\n",
        "#                                     activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkMj7KzN8Npe"
      },
      "source": [
        "def build_model(hp):\n",
        "\n",
        "\n",
        "  model = keras.Sequential()\n",
        "  model.add(Embedding(embedding_matrix.shape[0],embedding_matrix.shape[1],weights=[embedding_matrix],input_length=src_length,trainable=True))\n",
        "  # 256 is best neuron\n",
        "  neurons=hp.Int('neurons',min_value=64, max_value=312, step=64)\n",
        "  model.add(GRU(neurons))\n",
        "  model.add(RepeatVector(tar_length))\n",
        "  model.add(GRU(neurons, return_sequences=True))\n",
        "  \n",
        " #providing the range for hidden layers  \n",
        "  for i in range(hp.Int('num_of_layers',2,4)):         \n",
        "      #providing range for number of neurons in hidden layers\n",
        "      model.add(Dense(units=hp.Int('num_of_neurons'+ str(i),min_value=64,max_value=312,step=64),activation='relu'))\n",
        "\n",
        "\n",
        "  hp_drop=hp.Choice('dropout',values=[0.1,0.2,0.3])\n",
        "  model.add(Dropout(hp_drop))\n",
        "  model.add(TimeDistributed(Dense(tar_vocab_size,activation='sigmoid')))\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "      loss=\"categorical_crossentropy\",\n",
        "      metrics=[\"accuracy\"]\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jVRI99c8hQy",
        "outputId": "268586ae-5a92-479d-b918-f341c446ecb3"
      },
      "source": [
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective=kt.Objective('val_accuracy',direction='max'),\n",
        "    max_trials=4)\n",
        "# stop_early=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10,restore_best_weights=True)\n",
        "stop_early=tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max',patience=10,restore_best_weights=True)\n",
        "\n",
        "\n",
        "tuner.search(trainX,trainY, epochs=100,validation_data=(testX,testY))\n",
        "best_model = tuner.get_best_models()[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3 Complete [00h 27m 27s]\n",
            "val_accuracy: 0.6890000104904175\n",
            "\n",
            "Best val_accuracy So Far: 0.7261428833007812\n",
            "Total elapsed time: 01h 21m 25s\n",
            "\n",
            "Search: Running Trial #4\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "neurons           |256               |192               \n",
            "num_of_layers     |3                 |2                 \n",
            "num_of_neurons0   |128               |64                \n",
            "num_of_neurons1   |128               |64                \n",
            "dropout           |0.1               |0.1               \n",
            "learning_rate     |0.0001            |0.0001            \n",
            "num_of_neurons2   |192               |None              \n",
            "num_of_neurons3   |64                |None              \n",
            "\n",
            "Epoch 1/100\n",
            "250/250 [==============================] - 21s 73ms/step - loss: 5.6542 - accuracy: 0.6228 - val_loss: 3.5781 - val_accuracy: 0.6273\n",
            "Epoch 2/100\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 3.1315 - accuracy: 0.6281 - val_loss: 2.8348 - val_accuracy: 0.6273\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 2.6556 - accuracy: 0.6276 - val_loss: 2.2816 - val_accuracy: 0.6273\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - 18s 70ms/step - loss: 2.1828 - accuracy: 0.6388 - val_loss: 2.1558 - val_accuracy: 0.6727\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 2.0724 - accuracy: 0.6673 - val_loss: 2.1084 - val_accuracy: 0.6690\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 2.0280 - accuracy: 0.6680 - val_loss: 2.1016 - val_accuracy: 0.6694\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 2.0010 - accuracy: 0.6692 - val_loss: 2.0828 - val_accuracy: 0.6699\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 1.9845 - accuracy: 0.6695 - val_loss: 2.0675 - val_accuracy: 0.6690\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 1.9626 - accuracy: 0.6700 - val_loss: 2.0562 - val_accuracy: 0.6706\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - 17s 70ms/step - loss: 1.9250 - accuracy: 0.6720 - val_loss: 2.0056 - val_accuracy: 0.6734\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - 17s 70ms/step - loss: 1.8897 - accuracy: 0.6752 - val_loss: 1.9892 - val_accuracy: 0.6747\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - 18s 72ms/step - loss: 1.8669 - accuracy: 0.6775 - val_loss: 1.9714 - val_accuracy: 0.6774\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - 18s 72ms/step - loss: 1.8441 - accuracy: 0.6809 - val_loss: 1.9715 - val_accuracy: 0.6829\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - 18s 73ms/step - loss: 1.8250 - accuracy: 0.6831 - val_loss: 1.9538 - val_accuracy: 0.6867\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - 18s 72ms/step - loss: 1.8067 - accuracy: 0.6857 - val_loss: 1.9476 - val_accuracy: 0.6869\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - 18s 71ms/step - loss: 1.7915 - accuracy: 0.6873 - val_loss: 1.9337 - val_accuracy: 0.6884\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - 18s 71ms/step - loss: 1.7723 - accuracy: 0.6886 - val_loss: 1.9266 - val_accuracy: 0.6899\n",
            "Epoch 18/100\n",
            "190/250 [=====================>........] - ETA: 3s - loss: 1.7587 - accuracy: 0.6896"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4vbIdiK8Nrq"
      },
      "source": [
        "# Tuner Models Summary\n",
        "\n",
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38hs4T-6J5Hb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee34605-46fa-429b-ce0f-9cbddcbac421"
      },
      "source": [
        "# Best Parameter\n",
        "# learning rate: 0.001\n",
        "# Dropout rate: 0.3\n",
        "# Neurons: 256"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ]
        }
      ]
    }
  ]
}